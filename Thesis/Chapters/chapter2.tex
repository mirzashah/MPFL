% Chapter 2 - Master's Thesis 
% Mirza A. Shah
\chapter{Background Concepts}
This chapter introduces some concepts that were used in implementing MPFL. Connections are drawn between these concepts and will eventually be referenced in subsequent chapters.

\section{The Relationship Between Computation and Artificial Intelligence}
The kernel of computer science lies in the theory of computation. No statement summarizes the power and potential of computation more so than the \Definition{Church-Turing Thesis} that roughly states \textit{any effectively calculable function can be computed by a Turing machine, or one of its [Turing-equivalent] equivalents}. An \Definition{effectively calculable function} is a function that can be represented as a sequence of discrete steps, or in other words, as an algorithm. A \Definition{Turing machine} is a hypothetical digital computer that has infinite memory, and [supposedly] can compute any effectively calculable decidable function using only a finite portion of its memory. A \Definition{Turing-equivalent} computer is a computer that can compute the same set of functions as a Turing machine \citep{sipser:computationtheory}. Typical everyday computers, such as a home personal computer, would be considered Turing-equivalent if they had infinite amounts of memory. However, since any decidable computation uses a finite, bounded amount of memory, one can assume that if we cannot run an algorithm today because of memory or compute time constraints, we maybe able to in the future due to technological advances in computer memory and speed. Even if we cannot compute in the present day, there maybe suboptimal approximations that are sufficient. 

Even though there is no formal proof for the Church-Turing Thesis, all evidence to date hints at it being true. Many believe that all processes in our universe are effectively calculable, even though this a subject of debate. If this somehow turned out to be true, it would imply (through the Church-Turing Thesis) that our universe may simply just be a computer program in execution (as depicted by many works of science fiction)\footnote{\textit{Digital physics} is the area of physics and cosmology that believes that the universe is describable as computable data. \textit{Pancomputationalism} is the belief that the universe is simply a digital computer in execution.} or could even be simulated on a computer given sufficient amounts of memory and time. At the same time, it is an exciting prospect that \textit{anything is possible through computation}. This includes being able to create intelligent and sentient beings in our own image as mere computer algorithms.

\Definition{Artificial intelligence} (or \Definition{AI}) is the branch of computer science dedicated to creating such artificial beings. Though the idea of \textit{computing the universe} may seem far out to many, the idea of at least being able to mimic the mind through computation seems much more plausible. The idea that the \textit{mind is a computer} has now spread to other fields such as neuroscience, psychology, physics, philosophy, linguistics, and biology, and has become a de facto abstraction for reasoning about intelligence. In fact, all of these fields (including computer science and artificial intelligence) have merged into a single field called \Definition{cognitive science} dedicated to studying the human brain and intelligence.

\section{The Relationship Between AI and Autonomy}
The focus of this thesis is not so much on AI, but rather on \Definition{autonomous systems}. A system that is autonomous is an entity that can operate with limited or no intervention from another entity to achieve a set of goals. For example, automated assembly robots in factories, self-driving cars, autonomous mobile robots (which are the focus of this thesis), home security alarm systems, automatic thermostats, and even human beings are autonomous systems. It is not necessary for an autonomous system to be sentient, intelligent, or even complex. However, as we want more complex autonomy, we find that the system has to exhibit properties that are held only by \textit{intelligent}\footnote{The definition of \textit{intelligence} is controversial in the fields of artificial intelligence and cognitive science. The thesis purposely does not define this term. The reader is encourage to go with their intuitive notion of the term.} beings. Hence, creating autonomous systems can be thought of as an artificial intelligence problem.

When developing the software and hardware that act as the \textit{mind} of an autonomous system, one needs to look at various aspects of intelligence that AI researchers have attempted to model computationally. Key areas that are commonly highlighted in the AI literature and depicted in \RefPicture{components_of_intelligence.png} are:
\InsertPicture{components_of_intelligence.png}{0.25}{Artifacts of Intelligence}
\begin{itemize}
\item \textit{Sensing} - How does an intelligent entity perceive its environment and how it affects it? 
\item \textit{Data Fusion} - How does an intelligent entity interpret raw sensor data from multiple sources? How does it merge together information when sensors agree as well as contradict each other?
\item \textit{Knowledge Representation} - How does an intelligent entity store, relate, modify, update, and query knowledge to do its tasks?
\item \textit{Reasoning} - Using what is already known to an intelligent entity, how can it infer or deduce other facts?
\item \textit{Learning} - How can an intelligent entity adjust its behavior without intervention, factoring in previous experience and knowledge, to handle situations unforeseen to it?
\item \textit{Planning and Response} - How does an intelligent entity break down complex tasks into simpler steps to accomplish those tasks, while taking into account other tasks that must be done concurrently or in the future, while managing resource allocation?
\end{itemize}
Even though to date AI researchers have failed to create \Definition{strong AI} (also known as \Definition{general AI}), such as sentient computers that parallel or exceed human intelligence (i.e. \textit{the Singularity}), many useful tools that we take for granted in computer science and mathematics emerged from, were improved by, or were popularized by AI research. These include expert systems, constraint solvers/automatic theorem provers, natural language and symbolic processors, fuzzy logic, probabilistic logic, evolutionary computation (e.g. genetic algorithms, particle swarm optimization), database query engines, and neural networks \citep{russellnorvig:ai}.

\section{The Relationship Between Intelligence, Natural Languages, and Programming Languages} \LabelSection{relationshipintelligencelanguages}
In linguistics, psychology, anthropology, and sociology, the \Definition{Sapir-Whorf Hypothesis} (also known as the \Definition{Linguistic Relativity Hypothesis}) states that thought and behavior are a function of language \citep{kay:sapirwhorf}. Essentially it is saying that a person's thoughts and behavior are shaped by the expressive constructs of the language in which they articulate said thoughts (the level to which this is true remains controversial). For example, the \textit{Pirah\~{a}} tribe of South America has no numeric system, or even words for quantification such as \textit{each, many, some,} etc. There was never any need for any sort of quantification system in that tribe, hence it never evolved into the language  \citep{colapinto:piraha,vonbredow:livingwithoutnums}. At the same time, that tribe has not and will likely not discover the concepts and tools that come with more advanced mathematics as it is difficult to reason about or express those concepts without some sort of language (e.g. equations and variables). The extent to which the Sapir-Whorf Hypothesis is true is a highly controversial topic along researchers. However, there is a general consensus that language and thought are indeed intertwined; the controversy lies in the \textit{extent} to which language affects thought  \citep{carruthers:cogfunctionsoflanguage}. Another example supporting the Sapir-Whorf Hypothesis is that multiple studies have shown people who know more than one language have certain advantages in terms of intelligence such as better problem solving abilities and longer information retention in the face of neurological diseases such as Alzheimers \citep{bialystok-globallocal, bialystok-bilingualism, craik-alzheimerbilingual}. Psychologists suspect this is the case because people familiar with more than one language are able to reason about the same concepts from multiple perspectives as well as have redundant neural encodings of information sets.

The Sapir-Whorf Hypothesis applies to programming languages as well. Kenneth Iverson indirectly asserted this in his Turing Lecture on the importance of mathematical notation in reasoning about computation \citep{iverson:notationasatool}. The notion is quite obvious to a person that has programmed in multiple programming language paradigms. Take for example a computer program that takes as input an algebraic expression as a string and then outputs a reduced expression. To a programmer familiar with logic-based programming languages (such as Prolog), such a task is trivial; they would encode the reduction rules of algebra as logical predicates, and they would be done. The same problem in an imperative language such as C would likely baffle even the most adept imperative programmers; a parser would have to be created to break the expression into smaller subexpressions recursively, then each subexpression would have to be matched against the known reduction rules and evaluated, and finally the result of each evaluated subexpression would have to be passed up during the recursive unwind to the larger expression it constitutes to complete that expression's evaluation. Even if the programmer knew how to solve the problem, the amount of code it would take would be significantly more than that of a logic-based language. Though the Church-Turing Thesis implies that all Turing-equivalent models of computation are equivalent in what they can compute, it says nothing about how easy it is to express an algorithm in one type of computational model versus another. That is where the Sapir-Whorf Hypothesis comes into play; the reason why the example above favors the logic-based language is because the computational model of logic-based languages maps naturally to the problem of algebraic expression reduction, whereas the imperative computational model does not.  Like natural languages, it is common wisdom amongst programmers that learning more programming languages, particularly those in a new programming language paradigm, broadens one's programming abilities. It allows one to tackle a wider variety of problems more quickly. This is because it gives one multiple vantage points from which to view a concept, just as those who can speak multiple natural languages.

\section{The Relationship Between AI and Programming Languages} 
There is a rich history and relationship between artificial intelligence and programming languages. In the early days of AI, a common approach was to look at intelligence as a symbolic processing problem. The concept came from Allen Newell and Herbert Simon's famous \Definition{Physical Symbol System Hypothesis} which theorized that a physical symbol system (i.e. a closed formal system) is all that is really necessary to create any type of intelligent agent \citep{allennewell:symbolsystem}. What Newell and Simon mean is that thinking is really just symbolic manipulation where thoughts and actions can be encoded as a closed system of symbols and rules to manipulate said symbols (where rules themselves can be encoded as symbols). AI researchers refer to this symbolic approach to AI as \Definition{classical AI}, \Definition{Good Old-Fashioned AI} or simply \Definition{GOFAI} (pronounced ``go-fy") \citep{russellnorvig:ai}. This is in contrast to other paradigms such as the non-symbolic \Definition{connectionist} approach. Connectionists believe that intelligence can be modelled from a set of interconnected simple computational units that result in an emergent intelligence (e.g. neural networks). John McCarthy, another major AI pioneer and the one who first coined the term \textit{artificial intelligence}, was also a believer in the symbolic approach. This is one of the main reasons why he developed the famous programming language, \textit{Lisp}, which is designed for list processing. Symbolic expressions are just lists of symbols, hence Lisp is an excellent language for doing symbol processing. \textit{Prolog}, a logic-based language built years later, also was used heavily in AI applications as it allowed the creation of physical symbol systems in the form of logic rules (logic systems are an example of a physical symbol system). Lisp, its descendant Scheme, and Prolog were among the most dominant languages for implementing GOFAI algorithms for many years and are among the most common languages used to teach AI in universities today.

\subsection{Symbol Processing, Languages, and Semantics}
The notion of physical symbol systems is another way of looking at computation. In a sense, the physical symbol system hypothesis is restating the Church-Turing Thesis in that any physical process can be encoded as an algorithm that is computable by a Turing machine. The idea of symbol processing can be seen as creating a language interpreter, or in other words generating a programming language. Every symbolic expression can be manipulated based on a rule that is pattern matched against the expression's symbolic structure, and result in a new output expression. The rules encapsulate the meaning of each symbolic expression, and define what are known as the \Definition{semantics} of a programming language. Essentially, creating a programming language is analogous to creating a physical symbol system, such as an intelligent system.

\subsection{Knowledge Representation as Types}
As stated earlier, one of the challenging areas of artificial intelligence is knowledge representation; how does one represent, organize, access, and modify knowledge in a computer? One way to approach this problem is reasoning about knowledge in terms of a programming language's \Definition{type system}. Virtually every programming language has a notion of \Definition{types}. Types are classifiers for values/objects/expressions encountered in the language which restrict the operations that can be performed on or by those values/objects/expressions. A type system is a closed formal system (i.e. a physical symbol system) that defines all of those restrictions. An easier way to think about it is looking at types in our own world. When we look at objects, we distinguish them by their properties. For example, when we see a \textit{chair}, we assume certain things about it; it has a seat for us to sit on, and some mechanism to support it (such as four legs), and possibly a back to it. We can sit on it, stand on it, place other objects on it. We cannot use it to \textit{drive to work} or \textit{listen to music}; such operations do not make sense. In programming languages, types are valuable because they restrict invalid operations on values \citep{pierce:types}. They also allow the user to reason about their program more easily as types give structure and meaning to data, just as in the real world our mind ascribes types to give objects structure and meaning. Some of the tools that type systems provide which could be useful in implementing a knowledge representation scheme are listed in the following subsections.

\subsubsection{Composite types and abstract data types}
Almost every programming language has the ability for users to define their own types in the form of \Definition{composite types}. These types are typically constructed as a composition of the basic, or \Definition{primitive types}, provided by the language. Some languages take this a step further and allow the user to define a set of operations that can be performed by/on values of that type. These types are called \Definition{abstract data types} \citep{sethi:proglang}.

\subsubsection{Hierarchical Knowledge Representation Through Subtyping}
Many type systems allow the notion of \Definition{subtyping}, where data can be considered to have more than one type through a type hierarchy \citep{pierce:types}. For example, consider our real world example of the type \textit{chair}. Each of us probably imagines a different image when we think of a chair. Some may think of an office chair which swivels and sits on wheels. Others may think of a lawn chair, a living room chair, or a dining chair. Languages that support subtyping allow programmers to define a type such as \textit{lawn chair} as a subtype of \textit{chair}. This means a \textit{lawn chair} has all the properties of a general \textit{chair} (along with any operations if \textit{chair} is an abstract data type), as well as any additional properties unique to a \textit{lawn chair} (e.g. being waterproof). What makes this interesting is that in the course of the program, wherever a \textit{chair} is expected, a value of type \textit{lawn chair} can be used because \textit{a lawn chair is a chair}. However, a \textit{chair} is not necessarily a \textit{lawn chair}. Subtyping allows users to define knowledge in a hierarchical fashion, which turns out to be a valuable abstraction as cognitive scientists believe that the mind stores and relates many concepts in this manner \citep{minksy:societyofmind}.

\subsubsection{Reasoning Through Type Inferencing}
The previously mentioned aspects of intelligence, \textit{reasoning} and \textit{knowledge representation} are not disjoint concepts. One of the interesting features of some type systems is \Definition{type inferencing}. With type inferencing, a user does not have to explicitly classify data with a type. Rather the language interpreter/compiler can infer the type associated with a value based on how the value is used \citep{sethi:proglang, pierce:types}. For example, if someone was talking about baseball and said ``He hit \textit{it} way out of the park.", one can infer \textit{it} refers to a baseball. Programming languages with type inferencing work in a similar way. For example, in the language \textit{Standard ML} which has type inferencing, we can define a function \Code{addthree} that takes two values, and returns their sum plus 3: 
\begin{center}\Code{fun addthree(a, b) = a + b + 3}\end{center}
The compiler infers that \Code{a}, \Code{b}, and the returned value of \Code{addthree} must have type \textit{int} because of the ``\Code{+ 3}" at the end of the expression. Standard ML does not allow arithmetic addition between different types of numbers and because the `\Code{3}' is an integer the other operands must be integers. The way Standard ML is able to reason about the structure of data (i.e. knowledge) through inferencing makes it a potentially useful way to implement aspects of reasoning in an intelligent system.

\subsection{Hierarchical Error Handling through Exceptions}
Most computer programs of sufficient complexity require the programmer to handle errors or unforeseen situations. For example, in most languages, attempting to divide a number by zero will cause an error that may result in program termination. Some languages do not have any explicit or sophisticated form of error handling. For example, in C, a typical approach to seeing if a function caused any errors during its invocation is done by either checking the return value of the function or observing a global variable that is manipulated via a side-effect (e.g. \textit{errno} in C) caused by the function that contains the latest error code. A more interesting and structured approach to error handling that is found in many high-level languages is known as \Definition{exception handling} \citep{sethi:proglang}. Exception handling is a technique that separates the normal code from the error handling code. Whenever the programmer wants to indicate an error, they can \Definition{raise} (also called \Definition{throw}) an \Definition{exception}. Exceptions are entities in the language that represent an exceptional situation, such as an error. Programmers can indicate in the language which blocks of code they want to handle exceptions for by associating an \Definition{exception handler} with them. When an exception is raised, the program stops its normal path of execution and jumps to the nearest handler defined within the scope of the current call stack. Exceptions are typically values in a language, so they have associated with them data and/or a type. Not all handlers necessarily handle all errors, in which case the type information is used to match the nearest handler for that type within scope. If no handlers are found, the program will crash with some sort of \textit{unhandled exception} error. What is interesting about exception handling is that it creates a hierarchical way to deal with errors. As an analogy, if you have a problem with a home appliance, you may call the support line of the company that manufactures the appliance. If the customer service representative cannot help you, they may contact their supervisor and pass the problem up to them to handle. If they cannot help you, the problem will keep rippling up, perhaps to the owner of the company. If nobody helps you, your problem is not resolved like an unhandled exception. In a computer program, handling an error at the place it occurs may not be the best place to do it as somebody higher up in the call stack would have a better context of what to do.

\subsection{The Power of Abstraction - Sapir-Whorf Revisited}
When John McCarthy created Lisp, he did so because there was something lacking in then existing languages like FORTRAN. FORTRAN did not have the tools needed to express certain types of computations; in this case it lacked facilities for symbolic processing. Not having tools to deal with symbolic processing makes it difficult to create complex symbolic processors. This relates back to the Sapir-Whorf Hypothesis which argues that thought is a function of language. There are thousands of programming languages in existence; each designed to look at computational problems from a particular point of view. Some problems like the algebraic expression reduction example mentioned in \RefSection{relationshipintelligencelanguages} is trivial in Prolog versus C, whereas some problems in C are trivial compared to Prolog. Looking at computational problems as programming languages gives one the ability to create natural abstractions to express problems for a particular problem domain. 

\section{The Relationship Between AI and Domain-Specificity}
In the early days of AI, researchers who took the symbolic approach to AI assumed they could implement intelligence as a single, unified formal system. One of the famous attempts at doing this was the \textit{General Problem Solver (GPS)} program created by Newell and Simon \citep{newell:gps}. GPS allowed a user to specify any problem in any domain as a combination of a start state, state transformation rules, and goal states. GPS attempts to then create a path, using means-ends analysis, from the start state to the goal state via the state transformation rules. In other words, GPS builds a proof tree by applying rules sequentially until it can conclude the goal state can be reached. The problems with this concept were that 1) it was difficult to come up with all the rules for a particular problem and more importantly 2) as soon as there were a significant number of rules, the performance worsened.  The reason the performance went down was because of a phenomenon known as \Definition{rule explosion} that occurs in automated provers. GPS performs a depth-first search when constructing its proof. It has to examine every sequence of transformations from the start state that may lead to the goal state. If a path proves to be unfruitful, the prover has to backtrack and explore another path. The problem is that as more rules are added, the number of paths increases significantly. This increase is known as rule explosion. Rule explosion is an instance of the \Definition{search problem} in artificial intelligence: how does one efficiently examine the problem space to find a solution? For example, if a person wants to infer whether it is going to rain outside, they are going to check the sky to see if it is cloudy. However, another rule that person may have in their head that has the same antecedents such as \textit{if the sky is cloudy, it's a bad day to go tanning} would be completely irrelevant. GPS would not distinguish between \textit{if cloudy, it's going to rain} and \textit{if cloudy, it's bad to go tanning} as it does not distinguish between the relevance of rules and therefore does inefficient search. Examples like GPS have led many AI researchers to believe that the fundamental notion of implementing intelligence in a single, closed system is virtually impossible primarily because those systems cannot inherently do efficient searches. However, many still believe that the mind is implemented using a limited set of base constructs (i.e. a kernel) from which all human cognitive abilities emerge.

However, cognitive science has found evidence that the mind is a set of domain-specific entities interacting with each other. In other words, different portions of the brain are dedicated to doing very specific tasks. For example, in recent years neuroscientists isolated certain areas of the brain with neuron \textit{patches} dedicated purely to facial recognition \citep{buchen:patchesforface}. If these \textit{patches} were manipulated, we would not recognize people or perhaps recognize them as someone else. Evolutionary psychologists believe that everything the mind is stems from our basic instincts: to survive and reproduce through the process of natural selection and evolution. Evolutionary psychologists predominantly believe the human mind is a series of domain-specific entities dedicated to helping us simply survive and reproduce and were formed through the process of natural selection \citep{cosmides:invisiblehand,cosmides:originsofdomainspecific}. The idea of domain-specificity has spread to AI as well, mainly through the study of \Definition{agents}. Agents are individual entities (usually in the form of a computer program) that are responsible for handling some task, typically isolated to a narrow domain, on behalf of a user \citep{russellnorvig:ai, minksy:societyofmind}. Agents can talk to other agents to provide or receive services, and may have the power to take actions autonomously. One of the advantages of isolating problems to narrow domains is that the problem is much easier to reason about. Even using a formal system like GPS in a domain-specific situation may not be a bad idea as 1) the rule set is reasonably small enough to avoid rule explosion and 2) the domain is small enough to describe as a rule-based system. In general, by breaking up AI problems into smaller, domain-specific problems (such as agents), it is easier to reason about and create intelligent entities.

\section{The Relationship Between AI and Domain-Specific Programming Languages}
The ideas of domain-specificity and programming languages can be combined together to create a powerful approach to solving AI problems. Most mainstream programming languages can be classified as being general-purpose programming languages. These languages are designed to solve any general problem. Languages such as \textit{Ada, FORTRAN, C, C++, Java, Smalltalk, StandardML, Ruby, Perl, PHP, Python,} and \textit{OCaml} are classified as \textit{general-purpose languages}. This is in contrast to \Definition{domain-specific programming languages} or \Definition{DSPLs} which are meant to solve a problem for a particular problem domain such as \textit{Standard Query Language (SQL), Make, VHSIC Hardware Specification Language (VHDL), A Mathematical Programming Language (AMPL), regular expressions, LaTeX,} and \textit{Unix Shell Scripts} (e.g. \textit{Bash, Tsh}). These languages are not meant to solve every problem, but solve problems in their domain well, even though they may be Turing-equivalent.
Domain-specific languages are an advantageous approach to computation problems because they:
\begin{itemize}
\item \textit{Provide language constructs that naturally fit to a domain and its experts in order to make it easier to write programs for that domain.} For example, researchers at MIT designed a domain-specific programming language for microfluidic chips that could be programmed to perform chemical experiments for biology applications involving the mixing of chemicals within the chip \citep{thies:microfluidic}. The domain-specific language is used to describe the experiment to be performed: which chemicals to mix, which quantities and concentrations to use, and specifying which intermediate products should be used as reagents in subsequent reactions. The compiler for the language then generates a set of commands for a special microfluidic chip ``printer'' that in turn fabricates a disposable, single-use microfluidic chip. The chip is then connected to a special interface into which the chip inputs valves are connected to a supply of reactants and the experiment commences. Output chambers fill with final products as they are formed. The syntax of the language is tailored to that of biologists so that it is easier for them to build and reason about their experiments. A simple syntax that is specific to their domain does not require them to understand a more complex general-purpose language.
\item \textit{Provides verification at the domain-level to help better increase operational correctness and reduce development costs.} One form of this could be a domain-specific type system which would likely be able to do more rigorous checking of values in the language that could only be known at the domain level. For example, in the future, a hypothetical domain-specific programming language that instructs a hypothetical \textit{robot surgeon} how to perform surgery may be able to find mistakes that a surgeon might have forget such as \textit{Anesthetic X cannot be used on the patient because they have an allergy to it according to their patient history}. Mistakes like this could be caught by the interpreter or compiler for the language.
\item \textit{Increases reuse and better system-to-system interfacing through simplicity.} When a language is both expressive and simple to use, it will catch on faster and be more timeless than its analog in the form of a library-based application programmer interface. As the language stays relatively static, the implementation can change freely as long as it complies with the syntax and semantics of that language. Examples of languages that exemplify simplicity, elegance, and notable flexibility and ease in interfacing include:
\begin{itemize}
\item \textit{UNIX Shell Script} - A typical UNIX shell scripting language, such as \textit{Bash}, is a language that is designed to control the operating system and the programs it runs within an interpreter for that scripting language (e.g. the Bash shell is the interpreter for the Bash scripting language). UNIX shell scripts have a reputation to be able to do incredibly powerful things with very simple programs and have been in mainstream computing for over 30 years. 
\item \textit{Standard Query Language (SQL)} - Virtually every website on the Internet that has a database backend uses a \textit{relational database management system} or \textit{RDBMS} such as \textit{MySQL, Oracle, IBM DB/2, PostgreSQL} or \textit{Microsoft SQL Server}. RDBMSs are databases that store and query data based on a formal database model known as the \Definition{relational model}. SQL is a language designed for performing data queries in RDMBSs by incorporating the relational model in its underlying computational framework. Virtually every RDBMS utilizes or can utilize SQL as the user interface to query data. The language allows a developer to communicate with any database, regardless of how it is implemented, via an extremely simple language. Since 1979, relational databases have dominated the database world and the SQL domain-specific language takes a great role in that achievement.
\end{itemize}
\item \textit{Decoupling from a General-Purpose Programming Language.} A domain-specific language does not need to be tied to a specific programming language. For example, the most common way to talk to an SQL-based RDBMS from application code is by forming SQL statements as strings and sending them to the database's query engine via a database-provided API. The database then processes the query using the SQL interpreter/compiler within the query engine and returns the result.
\end{itemize}

\subsection{Libraries versus Domain-Specific Languages}
The purpose of domain-specific languages is to provide an interface to a user to solve some problem in a domain. However, most domain-specific interfaces do not come in the form of their own language, but rather more commonly come in the form of \Definition{libraries} that have an \Definition{application developer interface (API)} defined as constructs native and natural feeling to the language (such as a collection of classes in an object-oriented language, or a collection of procedures for procedural languages). Libraries provide a uniform and natural way for developers to utilize domain-specific tools in their programs that fits in with their thinking when developing applications. Even though libraries tend to be bound to a specific language, it is now common in modern software development to create \Definition{bindings} for popular libraries across many languages, making them even more accessible. Even without bindings, most languages provide a means to utilize code from other programming languages. It is also typically cheaper to develop software libraries versus languages as 1) creating programming languages is not a skill typical to software developers, 2) developing a language that really encapsulates the domain well is extremely difficult and 3) it is time consuming and burdensome to implement features of a language, such as its type system, properly. When attempting to build interfaces for a domain, deciding when to use a domain-specific language versus a library is a subjective choice based on what solution fits the problem best ~\citep{heering:dslsoftwareeng,mernik:whenhowdsl}. The following are some criteria from \citet{mernik:whenhowdsl} that would qualify a domain-specific language as a better approach to solving a problem versus a software library:
\begin{itemize}
\item A relatively simple language can tightly encapsulate the essential aspects of the domain.
\item The primary users will be people that are not comfortable with learning a general-purpose programming language to perform their desired tasks. The users are likely experts in the domain the DSPL is designed for.
\item Higher costs of developing a language instead of a library must be able to return investment by making it easier, cheaper, and faster to build and maintain solutions for a domain's problems.
\end{itemize}
\subsection{Looking at Software Development as a Programming Language Design Problem}
Looking at problems as the implementation of a programming language may result in better quality software. Programming language development is a rigorous process that requires everything to be specified (either formally or informally) to guarantee that what a programmer types in actually happens based on the semantics of the language \citep{mauw:languagedrivendesign}. The language developer is required to define a syntax for the language as well as static and runtime semantics. If these are not defined, then a programmer cannot write a program in the language, either because it's impossible or they cannot define what an expression in the language does. In other words, developing a programming language forces the language developer to think very hard about what the operational meaning of every construct in their language is. In the software development world, it is common when developing software (such as libraries) to ignore certain cases that the application may come across either out of time constraint, laziness, or oversight. It is common in the development world to see incomplete or frequently-changing APIs. These ignored cases are what cause a vast portion of software bugs. Tools to aid in developing software such as flow charts, block diagrams and the graphical \textit{Unified Modeling Language (UML)} provide vague specifications of how some software application is meant to operate. The tools used by programming language developers have no such ambiguity. The grammar (i.e. specification of the syntax) and the semantics of a programming language can be formally specified in mathematical notation. Any person familiar with those mathematical formalisms can then create an implementation of the language in the form of an interpreter or compiler that behaves exactly as the specification says it should. In contrast someone trying to describe the behavior of simple module of a program in UML could create dozens of UML diagrams, but still end up with an ambiguous specification.

\section{The Relationship between Intelligence, Planning, and Scheduling}
The specific focus of this thesis is on implementing a framework that focuses around planning, mentioned briefly in Chapter 1. \Definition{Planning} is the process of creating the steps necessary to achieve a set of \Definition{goals}. Goals are end states the autonomous system would like to be in by affecting itself and its surrounding world. For example the goal of being at some position $X$ is achieved when the autonomous system is actually located at position $X$. The resulting series of steps required by the autonomous system in order to achieve its goals are called a \Definition{schedule}. The autonomous system can break up a goal into several \Definition{subgoals} that may not be disjoint from each other in order to make it easier to develop its final schedule. For example, when you wake up each morning, you have a set of goals at hand that you would like to achieve. On a typical day, you wake up, decide you need to get ready for work, eat breakfast, then go to work, and eventually come home. You will think about what steps need to be taken to accomplish these goals, focusing more closely on the near term goals (like getting ready) rather than the long ones (like what to do when you get home), but keeping the long term ones in the back of your mind. You will also take into account your time requirements, if you have to be at work by 9 am, you attempt to schedule the getting ready, breakfast, and drive to work parts of your schedule to meet that requirements. If you're in a hurry, you may have to skip somethings, like eating breakfast which may have lower priority, than say, taking a shower. You  might find that even though you have planned things, the situation around you changes as the environment is dynamic, so you have to replan, and modify your schedule. For example, you may find your car does not start, so you need to get an alternate ride, which causes you to be late for work. Being late for work may alter your others plans as well, affecting the schedule you have laid out. You will then again have to replan and rebuild your schedule.

Planning is an important part of intelligence. It allows human beings to make decisions with care in order to maximize chances of successfully achieving goals. Any \textit{interesting} autonomous system needs to have some sort of mechanism to plan, whether it is reactive, deliberative, or somewhere in between.

\section{Planning as Processes}
In English, the word \Definition{process} refers to a series of actions/occurrences that result in some change and/or development in some environment. We use the term process to describe how physical events play through their execution. The way steel is manufactured, how rocks weather, how software is developed, why and how it rains, etc, can all be described as processes. In a sense, a process is similar to the notion of an algorithm. 

\subsection{Processes and Operating Systems}
The term \textit{process} has special meaning in computing, particular in the realm of operating systems. In a typical multitasking computer operating system, there are two important concepts: \Definition{programs} and \Definition{processes}. A computer program typically comes in the form of a binary file containing machine-level instructions that are executed by the computer. Alternatively it may come in the form of files containing source code or bytecode that is interpreted by a runtime interpreter. A process is an operating system abstraction that represents a running instance of a particular program and its associated execution context. This context comes in the form of a data structure, typically denoted as the \Definition{process control block (PCB)}, and contains data used by both the running program and the operating system \citep{silberschatz:osconcepts}. The PCB contains information about the process such as the process identifier, data stack, page table, register contents, stack pointer and program counter. When a user runs a program, the OS creates a new process by creating a new PCB and putting it on the OS' scheduler queue.

The purpose of having processes is two-fold. Having a context associated with every running program allows the creation of multiple running instances of the same program. One can run five instances of the same web browser; each is unique through the contents of its PCB. The other purpose is to make running programs manipulatable objects. The PCB captures the state of the entire machine for a running program, essentially providing a ``virtual machine environment" for  that program. In a multitasking operating system, processes do not realize they are sharing resources, such as the processor, with other processes. For example, in a single processor system, only one program can execute at a time on the processor. The OS allows many programs to run at the same time by utilizing timesharing. When a process has run for its given timeslice, it is put to sleep in order to allow another process to be revived and run. In order for this to work, the OS saves the machine state for the process that is to be put to sleep within its PCB. The OS then sets the real machine state (e.g. register contents, program counter, virtual-to-physical page mappings) to that contained within the incoming process' last saved PCB. The revived process continues running where it left off previously, not realizing that it was put to sleep and revived.

\subsection{Modeling Planning in Terms of Processes}
When developing planning systems, it can be helpful to think of concepts in terms of processes in the operating system sense. There are several ways to do this. For example, a goal can be thought of as a computer program, and \textit{an instance of an attempt to achieve that goal} can be thought of as a process associated with that goal. If you have the goal of going to several waypoints, you can break each waypoint into its own subgoal. A ``program" designed to solve the \textit{get to waypoint} problem can then be reused by creating several instances of that program (i.e. each represented as a process). The output of each process could represent part of the final schedule of what the vehicle is to do in order to achieve its goals. The input represents the goal specification along with the relevant pieces of perception information needed to solve the problem. Another way of looking at it is thinking of programs as individual planning systems, where a running instance of that program waits for goals to be input and outputs schedules to solve the problems for those goals. Multiple types and instances of these planning systems could communicate with each other to develop a final, cohesive schedule.\footnote{This concept is an example of a \textit{multi-agent system}. Agents were mentioned briefly earlier and represent a popular approach to developing intelligent systems.}

The advantage in thinking about planning in terms of processes is the same as the reason why multitasking operating systems reason about programs instances as processes: a process is a manipulatable and inspectable entity that can exist in parallel with other processes:
\begin{itemize}
\item The operating system can easily perform concurrent execution of several programs, even if its simulated through timesharing.
\item The operating system can provide certain degrees of validation, such as making sure processes do not violate their address space or perform invalid operations.
\item The operating system optimally schedules usage of limited system resources such as memory and processor time.
\item The operating system can provide a uniform and safe interface for processes to access system features through standard libraries and system calls.
\item The operating system can reason about concurrency more easily as each process encapsulates an isolated system.
\end{itemize}

If a planning system is thought of as an operating system that thinks of goal achievement attempts as processes, the planning system can leverage these advantages. The planning system can validate, to a certain extent, each goal achievement attempt, depending on the richness of the content of the PCB-like context associated with that attempt. It can make sure goals do not step on each other, provides a uniform interface to reach perception information, validate schedules, error handling, etc. Many goal attempt instances can be created from the same goal, reducing the amount of code needed.

\subsection{Processes and Programming Language Semantics}
Most mainstream languages encourage sequential, rather than concurrent, thinking --- particularly imperative languages. This is one of the reasons why many programmers have difficulty implementing multi-threaded applications: the Sapir-Whorf Hypothesis bites them as their language does not naturally incorporate concurrency into its computational model and style. 

One of the things that make it difficult to add constructs into a language for concurrency is that the semantic models people use are inherently sequential. For example, a popular type of semantics is \Definition{operational semantics}. This semantics describes how expressions are evaluated in a language in a sequential manner. In pure functional languages, any subexpression in an expression can be evaluated in any order\footnote{This is one of the consequences of the \textit{Church-Rosser Theorem}.}, meaning all the subexpressions could be evaluated in parallel, but is not an inherent requirement of the computational model. With certain types of operational semantics, such as \textit{big-step operational semantics}, you could imply that several subexpressions can be evaluated concurrently, but there is no explicit way of doing this.

Several types of mathematical models have been created for modeling concurrent systems, including programming languages with constructs for concurrent computation. Examples include the \textit{Actor model} and several \Definition{process calculi} also known as \Definition{process algebras} \citep{baeten:historyofprocessalg}. Process calculi are interesting in this context because semantics are expressed in terms of processes and the communications and relationships (such as sequential versus parallel execution) between those processes.

\section{Segue: The Big Picture}
This chapter has described several concepts; particularly intelligence, language, computation, domain-specificity, and autonomy. What makes these concepts important in this thesis is how they molded the thinking process in designing MPFL. The next chapter will describe what MPFL really is and how combining many of these concepts together can provide a powerful way to develop autonomous robots with sophisticated planning capabilities.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "mythesis"
%%% End: 
