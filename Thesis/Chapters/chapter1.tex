% Chapter 1 - Master's Thesis 
% Mirza A. Shah

\chapter{Introduction and Roadmap}

\section{The Future is Autonomous}
In our daily lives we take for granted countless tasks that are autonomously handled by non-human entities. Computers autonomously control our financial transactions, traffic lights, airline schedules, the amount of gasoline injected into our car's engine, and even the temperature of our homes. Everything from our home appliances to our breakfast cereal are made by tireless robots in factories overseen only by a handful of people. These computers and robots are all instances of \Definition{autonomous systems}. What qualifies a system as being \textit{autonomous} is subjective but in general it can be understood as some entity that can operate on its own, with little or no outside intervention, in order to accomplish some set of goals. Each living creature on this planet can be considered an autonomous system with the base goals of survival and reproduction. However, in the context of this thesis, autonomous system refers primarily to an artificial entity, such as a computer or a robot. We have increasingly become dependent on autonomous systems to do tasks for us because those tasks are simply too mundane, complex, time/energy consuming, costly, and/or dangerous if performed by a human. The inevitability is that in order for us to progress as a society, we will have to continuously delegate more work to autonomous systems.
  
\section{Autonomy and Robotics}
One of the most common examples of autonomous systems are \Definition{autonomous mobile robots}, also commonly referred to as \Definition{autonomous vehicles}. Autonomous mobile robots are robots that are both self-governing (i.e. autonomous) and can move around (i.e. mobile). These robots may operate aerially, on the ground, on water, and/or underwater.

\subsection{Commercial and Research Autonomous Robots}
\InsertPicture{roomba-rug.jpg}{0.20}{iRobot's \textit{Roomba} autonomous robot for vacuum cleaning. In addition to vacuuming, the Roomba is a popular platform for robot hobbyists and researchers due to its low price and hacker-friendly interface \citep{tribelhorn:evaluateroomba}. \textit{(Courtesy of iRobot Corporation)}}

\InsertPicture{robomow.jpg}{0.1}{Friendly Robotics' \textit{RoboMow} autonomous lawn mowing robot. \textit{(Courtesy of Friendly Robotics)}}

\InsertPicture{aibo.jpg}{0.7}{The Sony \textit{AIBO} is a robotic pet designed to mimic a dog. \textit{(Courtesy of Sony Corporation)}}

\InsertPicture{patrolbot.jpg}{1.4}{Adept MobileRobots' \textit{PatrolBot} is an autonomous robot for surveillance and security. Note the ultrasonic sensors (the copper mesh patches) circling the chassis and camera on top. \textit{(Courtesy of Adept MobileRobots)}}

\InsertPicture{honda_asimo.jpg}{0.45}{Honda's \textit{ASIMO} \citep{sakagami:asimo} is a humanoid robot; a robot meant to mimic the anatomy of a human. \textit{(Courtesy of Honda)}}

\InsertPicture{mars-rover.jpg}{0.45}{A depiction of one of the twin Mars rovers \textit{Spirit} and \textit{Opportunity} used in NASA's \textit{Mars Exploration Rover Mission}. \textit{Courtesy of NASA}}

Many commercial autonomous mobile robots have become mainstream. Some of the notable ones available in the market are iRobot Corporation's \textit{Roomba} vacuum cleaning robot (\RefPicture{roomba-rug.jpg}), Friendly Robotics' \textit{RoboMow} lawnmower robot (\RefPicture{robomow.jpg}), Sony Corporation's \textit{AIBO} robotic pet dog (\RefPicture{aibo.jpg}), and Adept MobileRobot's \textit{PatrolBot} security guard robot (\RefPicture{patrolbot.jpg}).  

Other robots, though not commercially available, are prototypes for potential commercial products.  The well-known Honda \textit{ASIMO} (\RefPicture{honda_asimo.jpg}) is a \textit{humanoid} robot that attempts to replicate human anatomy and movement. In 2003, history was made during NASA's \textit{Mars Exploration Rover Mission (MER)} when the twin autonomous rovers \textit{Spirit} and \textit{Opportunity} (\RefPicture{mars-rover.jpg}) landed on the surface of Mars and are still operational till this day.\footnote{In 2010, \textit{Spirit's} wheels became stuck in soil, turning it into a stationary platform, though it is still operational.}

\subsection{Military Autonomous Robots}
In the United States, autonomous and remote-controlled unmanned robotics are heavily used in military applications. In the last two decades, the United States Department of Defense (DoD) has become extremely interested in unmanned mobile robots and have invested heavily in the area. In recent years, General Atomics' \textit{Predator} unmanned aerial vehicle (UAV) (\RefPicture{predator-drone.jpg}) has been used heavily in combat in the U.S. \textit{War on Terror}. Autonomous underwater vehicles (AUV) such as iRobot's \textit{Seaglider} (\RefPicture{seaglider.jpg}) and Hydroid's \textit{REMUS} (\RefPicture{remus_600.jpg}) have been utilized in applications such as environmental monitoring, mine counter measures (MCM), and hydrographic surveys. Boston Dynamics' \textit{Big Dog} (\RefPicture{big_dog_boston_dynamics.jpg}), though not yet fielded, will eventually act as a robotic mule that can carry heavy supplies in out and of a battlefield. Robots such as iRobot's \textit{PackBot} (\RefPicture{pack_bot.jpg}) are used for explosive ordnance disposal applications(e.g. disabling a bomb), exploring dangerously damaged buildings for survivors, and reconnaissance in urban warfare. Using unmanned robots is cheaper than using manned vehicles, puts military personnel out of harm's way, and allows reaching places that humans cannot get to or safely navigate (e.g. collapsed building). DoD endeavors such as the Defense Advanced Research Projects Agency's (DARPA) \textit{Grand Challenge} and \textit{Urban Grand Challenge} (\RefPicture{darpa_grand_challenge_vehicle.jpg}) contests were widely publicized events aiming to push the state-of-the-art in autonomous vehicles. In fact, the United States Congress has passed laws that have set a mandate for at least one-third of all aerial vehicles to be unmanned by 2010 and at least one-third of all ground vehicles to be unmanned by 2015 \citep{roblaird:evolvingdodunmanned}.

\InsertPicture{predator-drone.jpg}{0.15}{General Atomics' \textit{Predator} aerial unmanned vehicle drone \textit{(Courtesy of General Atomics)}}

\InsertPicture{seaglider.jpg}{0.35}{iRobot's \textit{Seaglider} autonomous underwater vehicle \citep{eriksen:seaglider} utilizes buoyancy changes to \textit{fly} through the water in contrast to traditional propeller-based thrust \textit{(Courtesy of iRobot Corporation)}}

\InsertPicture{remus_600.jpg}{0.45}{Hydroid's \textit{REMUS 600} Autonomous Underwater Vehicle\textit{(Courtesy of Hydroid)}}

\InsertPicture{big_dog_boston_dynamics.jpg}{0.25}{Boston Dynamics' \textit{Big Dog} is an autonomous robot that helps carry heavy supplies across rough terrain like a mule. \textit{(Courtesy of Boston Dynamics)}}

\InsertPicture{pack_bot.jpg}{0.35}{iRobot's \textit{PackBot} is extremely popular in explosive ordnance disposal and other law enforcement/military applications. \textit{(Courtesy of iRobot Corporation)}}

\InsertPicture{darpa_grand_challenge_vehicle.jpg}{0.10}{A contender from the DARPA Grand Challenge}

\section{How Autonomous Robots Work}\LabelSection{howrobotswork}
Though autonomous robots differ greatly in their designs, their form factor, intended use, payload configuration and computer software, the following high-level algorithm describes the control loop followed by many autonomous robots:

\begin{enumerate}
\item Observe the world with onboard sensors
\item Perform data fusion on sensory data and update robot's world perception
\item Decide what to do next based on robot's goals and world perception, and issue commands to robot's actuators if necessary
\item Goto 1
\end{enumerate}

This algorithm is not meant to be an accurate depiction for all robots, but gives the reader a starting framework to reason about robot autonomy. In the robotics literature, this algorithm represents what is sometimes referred to as a \textit{sense-plan-act} or \textit{sense-model-plan-act} autonomy model \citep{alami:archforautonomy, brooks:newapproach}\footnote{In military terminology, the concept of an \textit{OODA (Observe-Orient-Decide-Act) loop} \citep{boyd:ooda} is almost identical to the \textit{sense-model-plan-act} concept. It is common amongst defense contractors working in the realm of autonomous systems to utilize the OODA metaphor.}. Human beings can also be thought to follow this algorithm in the way we function. 

\subsection{Sensors, Actuators, and Payloads}
Robots have varying types of \Definition{sensors} mounted on them for perceiving their surrounding world. Sensors for detecting objects includes infrared, ultrasonic (\RefPicture{ultrasonic_sensors.jpg}), laser, optical cameras, LIDAR (\RefPicture{lidar_equipped_robot.jpg}), sonar, and radar. They act as the eyes of a mobile robot. Other sensors are used for tracking the robot's position and orientation, such as global positional system (GPS), wheel encoders, magnetometers, accelerometers, altimeters, and inertial navigational systems. Environmental sensors are used for measuring properties of the environment, such as thermocouples for temperature and barometers for pressure. Sensors can overlap in responsibility. For example a sensor for pressure can be used to infer either the depth or altitude of the robot. Robots also may carry other types of \Definition{payloads}, such as a robotic arm (\RefPicture{irobot-create.png}) or grapple to interact with objects, or devices for communication such as a radio transmitter. Robots also have \Definition{actuators} that are devices to control the mechanical movements of vehicle itself, such as controlling a robotic arm, orienting an onboard camera, or controlling a servo that determines its heading and speed.

\InsertPicture{ultrasonic_sensors.jpg}{1}{Small ultrasonic sensors commonly used in hobbyist robotics}

\InsertPicture{lidar_equipped_robot.jpg}{0.25}{An autonomous robot with front mounted LIDAR}

\InsertPicture{irobot-create.png}{0.35}{iRobot \textit{Create} mobile robot with a robotic arm \textit{Courtesy of iRobot Corportation}}

\subsection{Data Fusion, Reasoning, and Perception}
The various data collected by the robot from its sensors are typically processed by a higher-level entity that performs \Definition{data fusion}, such as a digital signal processing (DSP) board and/or a general-purpose computer. Data fusion is the process of taking the clues provided by the various onboard sensors of the robot to help create or update a comprehensive, unified picture of the robot's worldview. This worldview is known as the robot's \Definition{perception}. In artificial intelligence terminology, this is also sometimes known as a \Definition{knowledge base} which is a place that holds all known facts. Likewise in military terminology, the term \Definition{situational awareness} is often used. For example, say the robot has two sensors for detecting an object. If both sensors suddenly indicate there is an object bearing $X$ degrees with a range of $Y$ meters, it is a good indicator there is something there. Sensors are not perfect, so sometimes one sensor may say something is there, while another says something is not there or something is there but at a different position. Sensor accuracy also depends on the distance from the object as well as environmental factors. The job of data fusion algorithms is to perform analysis of all sensor data while taking into consideration the accuracy of each sensor, cross-checking facts with other sensors, as well as previous sensor readings in order to create a unified picture of the world (hence the term \textit{fusion}).

The implementation of the knowledge base is important in terms of how knowledge is encoded, stored, indexed, queried, and modified. When human beings are in a given situation, their minds automatically and instantly \textit{prefetch} data that would be relevant/useful in that moment of time. For example, if one is in a car accident, their mind and body are flooded with thoughts and emotions that allow them to get out of that situation and continue on with their life (so as to continue to survive and reproduce). Thoughts may include experience from previous accidents, knowledge of what to do in a car accident (e.g. make sure everyone is alright, call the police, do not admit any fault), worries about how the car will be repaired or replaced, and tasks/meetings/obligations that are affected by the incident. Emotions, which themselves can be considered merely data, may also be invoked by the release of bodily chemicals and neurotransmitters such as adrenaline and dopamine. In the car accident analogy, emotions may include fear and caution so as to create conditions that improve probability of success. The mind reacts so quickly because of the way data is encoded within the brain;  facts are not merely stored but they have links between them. In a robot, we attempt to mimic this using tools from computer science. Memory is our physical storage. Thoughts are encoded in the form of programming language constructs and data structures (e.g. records, tuples, abstract data types, classes/objects, linked lists, arrays, binary trees). Thoughts can be cross-indexed via indexes (a common technique in databases) which themselves are metadata in the form data structures (e.g. hash tables, B-trees). Data can also be quickly recalled via data caching and exploiting data locality principles, which is of course a deeply-studied subject in computer science.

The perception/knowledge base of a robot can contain more information than what is processed by data fusion. Just like humans, robots can also infer/deduce other facts with some level of certainty. This act of acquiring new information with already known information via deduction and inference is known as \Definition{reasoning}. For example, if a person sees that it is gray and cloudy outside, they can infer that there is a reason chance of rainfall based on past experience. If thunder is heard, it increases the likelihood even further as it is rare to have thunder without rain. Once drops of rain start falling, a person can infer that it is indeed `actually raining'.  The ability to draw new facts from existing ones is important ability for both humans and autonomous robots in order to interact with world.

\subsection{Planning and Response}
Even though human beings still ponder the purpose of their existence and lives, the purpose of existence for an autonomous robot is very simple: do what is necessary to achieve a set of \Definition{goals} desired by the robot's user. A goal is a desired state the robot attempts to achieve. For example, a robot may have the goal of reaching some position $X$. The process of creating the necessary steps to achieve goals is known as \Definition{planning}, and is represented by step 3 of the algorithm mentioned at the beginning of \RefSection{howrobotswork}. The result of planning is a \Definition{schedule} which is a timestamped series of steps needed to be executed at the appropriate times to achieve a goal. For the example of ``go to point $X$", the planning phase may involve determining what heading the robot would need to take in order to get $X$ from its present location. The schedule resulting from planning would indicate the robot should move along the planned heading. The robot may need to continuously replan the heading to take into account obstacles and navigational sensor inaccuracy. When the robot eventually gets to point $X$, the robot's planning system will recognize the conditions for achieving the goal have been met, and mark it complete. The act of executing the steps in the plan is know as the \Definition{response} of the vehicle. When a robot completes all of its goals via its responses, it has served its purpose. Goals may also be accompanied by \Definition{constraints} such as time or energy usage that the robot must stay within when attempting to achieve its goals.

Planning is difficult to develop in an autonomous robot as there are many things to consider:

\begin{itemize}
\item What are the smaller steps that need to be taken by the robot to achieve a goal? What steps have already been taken?
\item If the robot has two or more goals that, if attempted, would overlap in some resource usage (such as time), how does it interleave these tasks together? If they cannot be interleaved because they conflict, how is that handled?
\item How does the robot plan for goals that have interdependencies and require collaboration?
\item If the robot does not think it can perform the tasks needed to achieve some goals, for reasons such as hardware failures or resource constraints, how does it handle that? What if later on it changes its mind and believes it can handle the task?
\item If the robot plans on doing other things in the future, how does that affect its decisions now? (e.g. if a certain amount of a resource is needed to do a future task, will there be enough of that resource available when the task commences?)
\item How does the robot plan when working with other robots each with its own goals as well as shared goals across robots?
\item How does the robot deal with incomplete or uncertain information when planning?
\item Is the robot planning optimally? As there are often multiple ways to solve a problem, one way may be better in terms of resource usage (e.g. takes less time, uses less power).
\end{itemize}

\subsubsection{Reactive versus Deliberative Planning}
Some autonomous robots have extremely simple planning systems, such as iRobot Corporation's \textit{Roomba}, whose main goal is \textit{to obtain clean carpets}. It moves about a room in straight lines and when it hits a wall, it moves in a random direction until it hits another wall. Sensors underneath the robot measure dirt level of the carpet. If the sensor detects nothing after a prolonged period of time, the carpet is considered clean. The planning does not take into consideration what areas have been visited, which places are likely to have dirt, etc. Even though the Roomba works in a fully autonomous fashion, its simplistic planning system results in vacuuming times of hours for a single room, versus the mere minutes it would take a human to do it. Other robots, such as the famous NASA Mars exploration rovers, \textit{Spirit} and \textit{Opportunity}, can do more sophisticated things such as path planning to avoid obstacles, as well as filtering of sensory input to send back only the most interesting and relevant telemetry back to Earth (as bandwidth is a limited resource). The planning system of a robot can be described as lying somewhere in a continuous spectrum between being \Definition{reactive} or being \Definition{deliberative} (\RefPicture{reactivevsdeliberative.png}). Even though the definition is subjective, reactive systems are those that do not take into account previous decisions or anticipate into the future what is to be done when making their next move, making the decision process very simple. Deliberative systems are those that try to take into account information about past moves, as well as try to anticipate what needs to be done in the future. Reactive systems are simpler to implement and debug, but are limited in the scope of what their autonomy can do. The Roomba is an excellent example of a reactive system. Likewise, the Mars rovers are comparatively more deliberative than the Roomba as they \textit{put more thought} into the way they make their decisions.
\InsertPicture{reactivevsdeliberative.png}{0.50}{The reactive-deliberative spectrum of robotic planning. (Image recreated from \citet{arkin:behaviorbasedrobotics})}

\subsection{Learning}
\Definition{Learning} is the ability for a robot to reason about past experience (e.g. previous sensor readings, current progress in achieving goals, etc) when attempting to achieve goals in order to modify data fusion, reasoning, and planning/response algorithms to increase the probability of achieving current and future goals. For example, a robot with a robotic arm may know how to open doors with door knobs. However, if it encounters a door with a latch, it may not know how to open it. If the robot could somehow understand that not all doors open in the same way and figure out a way to open the door, it could \textit{learn} how to open it.

Learning is a difficult problem. Current techniques for learning typically involve heavy use of probabilistic methods and are extremely limited in comparison to the learning abilities of a human being. Learning that is similar to that of humans almost requires the robot to have a ``a sense of purpose'' and an understanding of all the dynamics in the world and the consequences of all its actions. Perhaps it requires some form of metacognition and/or self-awareness.

\subsection{Putting it all Together}
\InsertPicture{howrobotworks.png}{0.40}{The \textit{sense-plan-model-act} algorithm is implemented as a set of independent agents communicating information.}
A typical high-level software architecture used for robotics autonomy is shown in \RefPicture{howrobotworks.png}. It provides a graphical form of the control loop algorithm mentioned at the beginning of \RefSection{howrobotswork}. The arrows in the diagram indicate data flows and each box can be thought of as an independent agent. These agents all run in parallel and asynchronously send and receive information from the other agents. It is common place in robotics to implement the software in a similar fashion as it takes advantage of multiprocessing, is more robust because if an agent fails the other agents continue running, and the design is more modular making it flexible to swap out agents.

\section{The Development and Challenges of Mission Planning in Mobile Robots}
Sensors, data fusion, perception, and planning/response are what make up the essential core autonomy of an autonomous robot. Though learning can improve performance of an automous system, it is not necessary to create a useful robot, though the robot will have limited creativity regarding how it can solve a problem and may give up sooner than necessary.

The data fusion, perception, and planning/response components of an autonomous robot's autonomy tend to be realized in the form of computer software that runs onboard the robot itself. The development of this software is a problem of not only computer science, but of software engineering as well. In general, software complexity and size grows with increasingly sophisticated robot autonomy. More sophisticated software engineering techniques must be employed to manage all the complexity.

This complexity has motivated the development of a myriad of software tools, libraries, frameworks, and application programmer interfaces (APIs) to help develop the autonomy of the robot. This thesis describes a new software framework called the \Definition{Modular Planning Framework and Language (MPFL)} that focuses on the planning aspect of autonomy development in mobile robots, particularly \Definition{autonomous underwater vehicles (AUVs)}, even though the concept could be generalized to any autonomous system. MPFL distinguishes itself from other autonomy frameworks as it looks at the problem of planning as a programming language. The framework is tied closely to a special domain-specific programming language for describing planning problems that inherently provides many interesting features including:
\begin{itemize}
\item Strong static and runtime verification mechanisms for mission execution integrity
\item A modular framework that allows component reuse across different robots
\item Naturally supports the development of heavily deliberative systems
\item Provides an elegant way of handling system and planning failures through exception handling
\end{itemize}

\section{Roadmap}
Chapter 2 gives some high level background concepts from various disciplines that inspired the design of MPFL. Chapter 3  gives an overview of MPFL. Chapter 4 describes MPFL's programming language: the \textit{Mission Specification Language (MSL)}. Chapter 5  explains how MPFL is utilized and how its works internally. Chapter 6 discusses a proof-of-concept demonstration system that implements the complete autonomy of a simulated robot that uses MPFL. Finally, Chapter 7 is an epilogue that provides analysis of the strengths and weaknesses of MPFL and areas of future research to improve the concept.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "mythesis"
%%% End: 
